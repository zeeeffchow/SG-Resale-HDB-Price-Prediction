{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8025a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Loading OneMap token\n",
    "load_dotenv()\n",
    "ONEMAP_API_TOKEN = os.getenv(\"ONEMAP_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7712b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "resale_1990_1999 = pd.read_csv('./data/raw_data/resale_1990_1999.csv')\n",
    "resale_2000_2012 = pd.read_csv('./data/raw_data/resale_2000_2012feb.csv')\n",
    "resale_2012_2014 = pd.read_csv('./data/raw_data/resale_2012mar_2014dec.csv')\n",
    "resale_2015_2016 = pd.read_csv('./data/raw_data/resale_2015jan_2016dec.csv')\n",
    "resale_2017_2025 = pd.read_csv('./data/raw_data/resale_2017_2025.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f48ba07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['month', 'town', 'flat_type', 'block', 'street_name', 'storey_range',\n",
      "       'floor_area_sqm', 'flat_model', 'lease_commence_date', 'resale_price'],\n",
      "      dtype='object')\n",
      "Index(['month', 'town', 'flat_type', 'block', 'street_name', 'storey_range',\n",
      "       'floor_area_sqm', 'flat_model', 'lease_commence_date', 'resale_price'],\n",
      "      dtype='object')\n",
      "Index(['month', 'town', 'flat_type', 'block', 'street_name', 'storey_range',\n",
      "       'floor_area_sqm', 'flat_model', 'lease_commence_date', 'resale_price'],\n",
      "      dtype='object')\n",
      "Index(['month', 'town', 'flat_type', 'block', 'street_name', 'storey_range',\n",
      "       'floor_area_sqm', 'flat_model', 'lease_commence_date',\n",
      "       'remaining_lease', 'resale_price'],\n",
      "      dtype='object')\n",
      "Index(['month', 'town', 'flat_type', 'block', 'street_name', 'storey_range',\n",
      "       'floor_area_sqm', 'flat_model', 'lease_commence_date',\n",
      "       'remaining_lease', 'resale_price'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(resale_1990_1999.columns)\n",
    "print(resale_2000_2012.columns)\n",
    "print(resale_2012_2014.columns)\n",
    "print(resale_2015_2016.columns)\n",
    "print(resale_2017_2025.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c1632a",
   "metadata": {},
   "source": [
    "There is an extra column 'remaining_lease' from years 2015 onwards. To handle this, we can either do one of the following:\n",
    "<ol>\n",
    "<li>Calculate the remaining lease for the previous years and append that to the previous years' dataframe.</li>\n",
    "<li>Drop the remaining lease column for 2015 onwards</li>\n",
    "<li>Use only data from 2015 onwards</li>\n",
    "</ol>\n",
    "\n",
    "<b>Option 3</b> was chosen for two reasons:\n",
    "- In context, remaining number of years of a lease is likely a significant factor of consideration for home buyers, hence it would not make sense to drop this column.\n",
    "- Between years 2015 and 2025, there is sufficient data for us to work with (~250k records) for prediction.\n",
    "\n",
    "Hence, we will only be making use of data from 2015 onwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada0bae3",
   "metadata": {},
   "source": [
    "<h2>Checking for NaN</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85747bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month                  False\n",
      "town                   False\n",
      "flat_type              False\n",
      "block                  False\n",
      "street_name            False\n",
      "storey_range           False\n",
      "floor_area_sqm         False\n",
      "flat_model             False\n",
      "lease_commence_date    False\n",
      "remaining_lease        False\n",
      "resale_price           False\n",
      "dtype: bool\n",
      "month                  False\n",
      "town                   False\n",
      "flat_type              False\n",
      "block                  False\n",
      "street_name            False\n",
      "storey_range           False\n",
      "floor_area_sqm         False\n",
      "flat_model             False\n",
      "lease_commence_date    False\n",
      "remaining_lease        False\n",
      "resale_price           False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(resale_2015_2016.isnull().any())\n",
    "print(resale_2017_2025.isnull().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a130c7",
   "metadata": {},
   "source": [
    "We note that there are no missing values in the data from 2015 to 2025, which is a good start. There are no rows for us to drop nor fill in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f56608",
   "metadata": {},
   "source": [
    "<h2>Checking dtypes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7710540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month                   object\n",
       "town                    object\n",
       "flat_type               object\n",
       "block                   object\n",
       "street_name             object\n",
       "storey_range            object\n",
       "floor_area_sqm         float64\n",
       "flat_model              object\n",
       "lease_commence_date      int64\n",
       "remaining_lease          int64\n",
       "resale_price           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resale_2015_2016.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8041d76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month                   object\n",
       "town                    object\n",
       "flat_type               object\n",
       "block                   object\n",
       "street_name             object\n",
       "storey_range            object\n",
       "floor_area_sqm         float64\n",
       "flat_model              object\n",
       "lease_commence_date      int64\n",
       "remaining_lease         object\n",
       "resale_price           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resale_2017_2025.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5de39f",
   "metadata": {},
   "source": [
    "Note that remaining_lease is of a different dtype for both dataframes. This is because the format from 2017 onwards captures both the number of years and months left, while in 2015-2016 only the number of years is captured.\n",
    "\n",
    "To standardize this, we will round down the 2017–2025 values to the nearest whole year and convert the column to int64 to represent the remaining lease in full years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c12c5851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing months from remaining_lease for 2017-2025 to nearest floor by year\n",
    "resale_2017_2025['remaining_lease'] = resale_2017_2025['remaining_lease'].str.extract(r'(\\d+)\\s+years').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d250f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249323, 11)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([resale_2015_2016, resale_2017_2025], ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67952b91",
   "metadata": {},
   "source": [
    "<h1>Feature Engineering</h2>\n",
    "\n",
    "Here, we will be attempting to bring in distance based features to local amenities and facilities as these are factors that play major factor when home buyers consider a house.\n",
    "\n",
    "We will be engineering the features that capture the block's distance to nearest hawker centre, school, hospital, childcare, kindergarten, park, community club, and distance to Downtown Core. We will also be capturing the number of each of these amenities within 1km of the block.\n",
    "\n",
    "To do so, we deploy the help of OneMapAPI, as well as data that we could get from data.gov.sg."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f588a05",
   "metadata": {},
   "source": [
    "<h2>Geocoding Blocks</h2>\n",
    "\n",
    "We will first begin by geocoding the blocks to find their coordinates, based on OneMapAPI's search method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbcfeed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address -> (lat, lon) cache\n",
    "address_cache = {}\n",
    "\n",
    "# Helper function to call OneMapAPI's Search method\n",
    "def geocode_address(address):\n",
    "    if address in address_cache:\n",
    "        return address_cache[address]\n",
    "    \n",
    "    url = \"https://www.onemap.gov.sg/api/common/elastic/search\"\n",
    "    headers = {\"Authorization\": f\"Bearer {ONEMAP_API_TOKEN}\"}\n",
    "    params = {\n",
    "        'searchVal': address,\n",
    "        'returnGeom': 'Y',\n",
    "        'getAddrDetails': 'Y',\n",
    "        'pageNum': 1\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params, timeout=10)\n",
    "        time.sleep(0.5)\n",
    "        if response.status_code == 200:\n",
    "            results = response.json().get(\"results\")\n",
    "            if results:\n",
    "                lat = float(results[0]['LATITUDE'])\n",
    "                lon = float(results[0]['LONGITUDE'])\n",
    "                address_cache[address] = (lat, lon)\n",
    "                return (lat, lon)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[ERROR] Address: {address} | {e}\")\n",
    "    address_cache[address] = (None, None)\n",
    "    return (None, None)\n",
    "\n",
    "# Helper function to call geocode_address in batches to avoid timeouts, on top of sleeping in geocode_address.\n",
    "def batch_geocode(df, address_column='full_address'):\n",
    "    coords = []\n",
    "    for addr in tqdm(df[address_column]):\n",
    "        coords.append(geocode_address(addr))\n",
    "    return coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71f0b973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating full_address column by concatenating block + street_name\n",
    "df['full_address'] = df['block'].astype(str) + ' ' + df['street_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455c260e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6446/9682 [2:38:03<9:14:27, 10.28s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Address: 146 RIVERVALE DR | HTTPSConnectionPool(host='www.onemap.gov.sg', port=443): Read timed out. (read timeout=10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7590/9682 [2:51:55<2:00:13,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Address: 308 TAMPINES ST 32 | HTTPSConnectionPool(host='www.onemap.gov.sg', port=443): Read timed out. (read timeout=10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9682/9682 [3:16:09<00:00,  1.22s/it]  \n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate addresses to reduce API calls\n",
    "df_unique = df[['full_address']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Run batch geocoding\n",
    "# WARNING!!!!! This took me 3.5 hours to run.\n",
    "df_unique['coordinates'] = batch_geocode(df_unique, 'full_address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfbb682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge coordinates back to the original dataframe\n",
    "df = df.merge(df_unique, on='full_address', how='left')\n",
    "\n",
    "# Split into lat/lon\n",
    "df[['latitude', 'longitude']] = pd.DataFrame(df['coordinates'].tolist(), index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5563d94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month                  False\n",
      "town                   False\n",
      "flat_type              False\n",
      "block                  False\n",
      "street_name            False\n",
      "storey_range           False\n",
      "floor_area_sqm         False\n",
      "flat_model             False\n",
      "lease_commence_date    False\n",
      "remaining_lease        False\n",
      "resale_price           False\n",
      "full_address           False\n",
      "coordinates            False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16504fbb",
   "metadata": {},
   "source": [
    "<h3>Distance to nearest MRT Station & Number of MRT Stations within 1 km from the block.</h3>\n",
    "\n",
    "To do so, we will be using data scraped from https://spotters.sgtrains.com/guide-abbreviation, geocode the MRT Stations with OneMAPAPI, and then calculate the distance between the MRT station and the block using scipy.spatial's cKDTree module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0234ed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Load MRT geocoded stations\n",
    "df_mrt = pd.read_csv(\"geocoded_mrt_stations.csv\")\n",
    "mrt_coords = df_mrt[['latitude', 'longitude']].to_numpy()\n",
    "\n",
    "# Drop rows with missing or infinite coordinates\n",
    "df = df.dropna(subset=['latitude', 'longitude'])\n",
    "df = df[np.isfinite(df['latitude']) & np.isfinite(df['longitude'])]\n",
    "\n",
    "# Now extract coordinates safely\n",
    "hdb_coords = df[['latitude', 'longitude']].to_numpy()\n",
    "\n",
    "# Build cKDTree for fast spatial lookup\n",
    "mrt_tree = cKDTree(mrt_coords)\n",
    "\n",
    "# Conversion factor: degrees to kilometers\n",
    "DEGREE_TO_KM = 111  # ~111km per degree on Earth's surface\n",
    "\n",
    "#Compute distance to nearest MRT\n",
    "distances, _ = mrt_tree.query(hdb_coords, k=1)\n",
    "df['distance_to_nearest_mrt_km'] = (distances * DEGREE_TO_KM).round(5)\n",
    "\n",
    "#Compute number of MRT stations within 1km\n",
    "radius_km = 1\n",
    "radius_degrees = radius_km / DEGREE_TO_KM\n",
    "neighbors_within_1km = mrt_tree.query_ball_point(hdb_coords, r=radius_degrees)\n",
    "df['num_mrt_within_1km'] = [len(neighbors) for neighbors in neighbors_within_1km]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cebc3a",
   "metadata": {},
   "source": [
    "<h3>Fetch OneMap themes</h3>\n",
    "\n",
    "Here, we will be using OneMapAPI's theme method to get the necessary themes for each amenity listed earlier, to calculate the distance to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_amenity_coordinates(queryname: str, token: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch amenity point data from OneMap Theme API based on queryname.\n",
    "    Extracts latitude and longitude from the 'LatLng' field.\n",
    "    \"\"\"\n",
    "    url = f\"https://www.onemap.gov.sg/api/public/themesvc/retrieveTheme?queryName={queryname}\"\n",
    "    headers = {\"Authorization\": token}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    srch_results = data.get(\"SrchResults\", [])[1:]\n",
    "\n",
    "    records = []\n",
    "    for item in srch_results:\n",
    "        try:\n",
    "            latlng = item.get(\"LatLng\")\n",
    "            if latlng:\n",
    "                lat_str, lon_str = latlng.strip().split(\",\")\n",
    "                lat = float(lat_str)\n",
    "                lon = float(lon_str)\n",
    "                name = item.get(\"NAME\", queryname)\n",
    "                records.append({\"name\": name, \"latitude\": lat, \"longitude\": lon})\n",
    "        except (ValueError, AttributeError):\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def add_distance_features(df_hdb: pd.DataFrame, df_amenity: pd.DataFrame,\n",
    "                          amenity_label: str, radius_km: float = 1.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds distance to nearest amenity and count within radius_km.\n",
    "    \"\"\"\n",
    "    # Build tree\n",
    "    amenity_coords = df_amenity[['latitude', 'longitude']].to_numpy()\n",
    "    hdb_coords = df_hdb[['latitude', 'longitude']].to_numpy()\n",
    "    tree = cKDTree(amenity_coords)\n",
    "\n",
    "    DEGREE_TO_KM = 111\n",
    "    radius_deg = radius_km / DEGREE_TO_KM\n",
    "\n",
    "    # Distance to nearest\n",
    "    distances, _ = tree.query(hdb_coords, k=1)\n",
    "    df_hdb[f'distance_to_nearest_{amenity_label}_km'] = (distances * DEGREE_TO_KM).round(5)\n",
    "\n",
    "    # Count within radius\n",
    "    neighbors = tree.query_ball_point(hdb_coords, r=radius_deg)\n",
    "    df_hdb[f'num_{amenity_label}s_within_{int(radius_km)}km'] = [len(n) for n in neighbors]\n",
    "\n",
    "    return df_hdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e56d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hawker Centres\n",
    "df_hawkers = fetch_amenity_coordinates(\"ssot_hawkercentres\", ONEMAP_API_TOKEN)\n",
    "df = add_distance_features(df_hdb=df, df_amenity=df_hawkers, amenity_label=\"hawker_centre\")\n",
    "\n",
    "# Libraries\n",
    "df_libraries = fetch_amenity_coordinates(\"libraries\", ONEMAP_API_TOKEN)\n",
    "df = add_distance_features(df_hdb=df, df_amenity=df_libraries, amenity_label=\"library\")\n",
    "\n",
    "# Hospitals\n",
    "df_hospitals = fetch_amenity_coordinates(\"moh_hospitals\", ONEMAP_API_TOKEN)\n",
    "df = add_distance_features(df_hdb=df, df_amenity=df_hospitals, amenity_label=\"hospital\")\n",
    "\n",
    "# Childcare\n",
    "df_childcares = fetch_amenity_coordinates(\"childcare\", ONEMAP_API_TOKEN)\n",
    "df = add_distance_features(df_hdb=df, df_amenity=df_childcares, amenity_label=\"childcare\")\n",
    "\n",
    "# Community Clubs\n",
    "df_kindergartens = fetch_amenity_coordinates(\"kindergartens\", ONEMAP_API_TOKEN)\n",
    "df = add_distance_features(df_hdb=df, df_amenity=df_kindergartens, amenity_label=\"kindergarten\")\n",
    "\n",
    "# Parks\n",
    "df_parks = fetch_amenity_coordinates(\"nationalparks\", ONEMAP_API_TOKEN)\n",
    "df = add_distance_features(df_hdb=df, df_amenity=df_parks, amenity_label=\"park\")\n",
    "\n",
    "# Community Clubs\n",
    "df_cc = fetch_amenity_coordinates(\"communityclubs\", ONEMAP_API_TOKEN)\n",
    "df = add_distance_features(df_hdb=df, df_amenity=df_cc, amenity_label=\"communityclub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b709f5e",
   "metadata": {},
   "source": [
    "<h3>Distance to nearest school and number of schools within 1km</h3>\n",
    "\n",
    "Here, we do the same approach for schools. The data is retrieved from https://data.gov.sg/datasets/d_688b934f82c1059ed0a6993d2a829089/view, that provides the general information of primary, secondary and pre-university schools. In which, we can find the postal code of the schools and use OneMapAPI's search method to geocode it.\n",
    "\n",
    "Lastly, we calculate the distance again using cKDTree, which is found in add_distance_features() we had defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geocode_postal_codes(df: pd.DataFrame, token: str, postal_col: str = \"postal_code\") -> pd.DataFrame:\n",
    "\n",
    "    geocoded = []\n",
    "\n",
    "    for postal_code_raw in df[postal_col].astype(str).unique():\n",
    "        postal_code = postal_code_raw.zfill(6)  # pad to 6 digits to prevent missing start 0 for postal codes that begin with 0\n",
    "\n",
    "        url = f\"https://www.onemap.gov.sg/api/common/elastic/search\"\n",
    "        params = {\n",
    "            \"searchVal\": postal_code,\n",
    "            \"returnGeom\": \"Y\",\n",
    "            \"getAddrDetails\": \"Y\",\n",
    "            \"pageNum\": 1\n",
    "        }\n",
    "        headers = {\"Authorization\": token}\n",
    "\n",
    "        try:\n",
    "            resp = requests.get(url, params=params, headers=headers)\n",
    "            resp.raise_for_status()\n",
    "            results = resp.json().get(\"results\", [])\n",
    "            if results:\n",
    "                lat = float(results[0][\"LATITUDE\"])\n",
    "                lon = float(results[0][\"LONGITUDE\"])\n",
    "                geocoded.append({\"postal_code\": postal_code, \"latitude\": lat, \"longitude\": lon})\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to geocode postal code {postal_code}: {e}\")\n",
    "        \n",
    "        time.sleep(0.1)\n",
    "\n",
    "    geo_df = pd.DataFrame(geocoded)\n",
    "    df[\"postal_code\"] = df[\"postal_code\"].astype(str).str.zfill(6)\n",
    "    return df.merge(geo_df, on=\"postal_code\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3572d8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      school_name postal_code  latitude   longitude\n",
      "0        ADMIRALTY PRIMARY SCHOOL      738907  1.442635  103.800040\n",
      "1      ADMIRALTY SECONDARY SCHOOL      737916  1.445891  103.802398\n",
      "2    AHMAD IBRAHIM PRIMARY SCHOOL      768643  1.433153  103.832942\n",
      "3  AHMAD IBRAHIM SECONDARY SCHOOL      768928  1.436060  103.829719\n",
      "4                  AI TONG SCHOOL      579646  1.360583  103.833020\n"
     ]
    }
   ],
   "source": [
    "# List of schools and information\n",
    "schools_df = pd.read_csv('Generalinformationofschools.csv')\n",
    "\n",
    "# Geocode schools by postal code\n",
    "geocoded_schools_df = geocode_postal_codes(schools_df, token=ONEMAP_API_TOKEN)\n",
    "\n",
    "print(geocoded_schools_df[['school_name', 'postal_code', 'latitude', 'longitude']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4171e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_distance_features(df_hdb=df, df_amenity=geocoded_schools_df, amenity_label=\"school\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a054a3",
   "metadata": {},
   "source": [
    "<h3>Distance to Downtown Core</h3>\n",
    "\n",
    "We will be including the distance to the Downtown Core area of Singapore (distance_to_downtown_core_km), since it is another significant factor of consideration for home buyers. As the downtown core is an area, we will be using the coordinates of Raffles Place MRT as the centroid of downtown core to calculate this distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "faf8fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raffles Place MRT (approx.)\n",
    "downtown_core_coords = (1.2831, 103.8515)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2372134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def add_distance_to_point(df, point_coords, label='downtown_core'):\n",
    "    DEGREE_TO_KM = 111\n",
    "    df[f'distance_to_{label}_km'] = df.apply(\n",
    "        lambda row: distance.euclidean((row['latitude'], row['longitude']), point_coords) * DEGREE_TO_KM,\n",
    "        axis=1\n",
    "    ).round(5)\n",
    "    return df\n",
    "\n",
    "# Apply to df\n",
    "df = add_distance_to_point(df, downtown_core_coords, label='downtown_core')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544ab71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('final_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1687aedf",
   "metadata": {},
   "source": [
    "The full dataset now contains the new features of distance to various amenities, number of such amenities within 1km, and the distance to downtore core. This dataset is saved to 'final_dataset.csv'.\n",
    "\n",
    "In the following section, we will attempt to preprocess the dataset, then create two copies of the dataset - one for linear modelling, another for tree-based modelling, before encoding the necessary attributes in appropriate ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf30c7a",
   "metadata": {},
   "source": [
    "<h1>Data Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d967fbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert month to datetime\n",
    "df['month'] = pd.to_datetime(df['month'])\n",
    "\n",
    "# Extract year and month\n",
    "df['year'] = df['month'].dt.year\n",
    "df['month_num'] = df['month'].dt.month\n",
    "\n",
    "# Drop columns not useful for modelling\n",
    "df = df.drop(columns=['block', 'street_name', 'full_address', 'coordinates', 'month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd83965",
   "metadata": {},
   "source": [
    "<h3>Creating df_linear for linear modelling</h3>\n",
    "\n",
    "For categorical attributes, we will be encoding them in various methods:\n",
    "- One-hot encoding is selected for 'town' and 'flat_model', as these attributes do not have meaningful order in the categories.\n",
    "- Ordinal encoding is performed for 'flat_type', as there is meaningful order in terms of number fo rooms and mean square area. We will be encoding them to values ranging from 1 to 7 (1-5 for 1-5 ROOMS, 6 for 'Executive' and 7 for 'Multi-Generation').\n",
    "- For 'storey_range', we will be taking the median of the range to convert it from a categorical attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a84b4898",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linear = df.copy()\n",
    "\n",
    "# One-hot encoding for town and flat_model, since they do not have meaningful order\n",
    "categorical_cols = ['town', 'flat_model']\n",
    "df_linear = pd.get_dummies(df_linear, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Ordinal encoding for flat_type, as there is meaningful order in terms of number of rooms and mean square area.\n",
    "flat_type_order = {\n",
    "    '1 ROOM': 1,\n",
    "    '2 ROOM': 2,\n",
    "    '3 ROOM': 3,\n",
    "    '4 ROOM': 4,\n",
    "    '5 ROOM': 5,\n",
    "    'EXECUTIVE': 6,\n",
    "    'MULTI-GENERATION': 7\n",
    "}\n",
    "df['flat_type_encoded'] = df['flat_type'].map(flat_type_order)\n",
    "df_linear.drop('flat_type', axis=1, inplace=True)\n",
    "\n",
    "# Story Median - we shall extract the median of storey_range to convert it from categorical to numerical.\n",
    "def extract_storey_median(storey_str):\n",
    "    try:\n",
    "        low, high = map(int, storey_str.split(' TO '))\n",
    "        return (low + high) / 2\n",
    "    except:\n",
    "        return np.nan  # handle unexpected formatting\n",
    "df_linear['storey_median'] = df_linear['storey_range'].apply(extract_storey_median)\n",
    "df_linear.drop('storey_range', axis=1, inplace=True)\n",
    "\n",
    "# Remove datetime and non-numeric fields if still present\n",
    "non_numeric_cols = df_linear.select_dtypes(include=[\"datetime64\", \"object\"]).columns\n",
    "df_linear = df_linear.drop(columns=non_numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "03ceb8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linear.to_csv('dataset_for_linear.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e3bb83",
   "metadata": {},
   "source": [
    "<h3>Creating df_tree for tree-based models</h3>\n",
    "\n",
    "For tree-based models like Random Forest, XGBoost, LightGBM, CatBoost, they can handle ordinal like numbers without assuming linear relationships. In fact, one-hot encoding might even reduce performance due to unnecessary feature splits.\n",
    "\n",
    "As such, we will simply be using LabelEncoder from sklearn to encode the categorical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb5f469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding for 'town':\n",
      "  ANG MO KIO → 0\n",
      "  BEDOK → 1\n",
      "  BISHAN → 2\n",
      "  BUKIT BATOK → 3\n",
      "  BUKIT MERAH → 4\n",
      "  BUKIT PANJANG → 5\n",
      "  BUKIT TIMAH → 6\n",
      "  CENTRAL AREA → 7\n",
      "  CHOA CHU KANG → 8\n",
      "  CLEMENTI → 9\n",
      "  GEYLANG → 10\n",
      "  HOUGANG → 11\n",
      "  JURONG EAST → 12\n",
      "  JURONG WEST → 13\n",
      "  KALLANG/WHAMPOA → 14\n",
      "  MARINE PARADE → 15\n",
      "  PASIR RIS → 16\n",
      "  PUNGGOL → 17\n",
      "  QUEENSTOWN → 18\n",
      "  SEMBAWANG → 19\n",
      "  SENGKANG → 20\n",
      "  SERANGOON → 21\n",
      "  TAMPINES → 22\n",
      "  TOA PAYOH → 23\n",
      "  WOODLANDS → 24\n",
      "  YISHUN → 25\n",
      "\n",
      "Encoding for 'flat_type':\n",
      "  1 ROOM → 0\n",
      "  2 ROOM → 1\n",
      "  3 ROOM → 2\n",
      "  4 ROOM → 3\n",
      "  5 ROOM → 4\n",
      "  EXECUTIVE → 5\n",
      "  MULTI-GENERATION → 6\n",
      "\n",
      "Encoding for 'flat_model':\n",
      "  2-room → 0\n",
      "  3Gen → 1\n",
      "  Adjoined flat → 2\n",
      "  Apartment → 3\n",
      "  DBSS → 4\n",
      "  Improved → 5\n",
      "  Improved-Maisonette → 6\n",
      "  Maisonette → 7\n",
      "  Model A → 8\n",
      "  Model A-Maisonette → 9\n",
      "  Model A2 → 10\n",
      "  Multi Generation → 11\n",
      "  New Generation → 12\n",
      "  Premium Apartment → 13\n",
      "  Premium Apartment Loft → 14\n",
      "  Premium Maisonette → 15\n",
      "  Simplified → 16\n",
      "  Standard → 17\n",
      "  Terrace → 18\n",
      "  Type S1 → 19\n",
      "  Type S2 → 20\n",
      "\n",
      "Encoding for 'storey_range':\n",
      "  01 TO 03 → 0\n",
      "  04 TO 06 → 1\n",
      "  07 TO 09 → 2\n",
      "  10 TO 12 → 3\n",
      "  13 TO 15 → 4\n",
      "  16 TO 18 → 5\n",
      "  19 TO 21 → 6\n",
      "  22 TO 24 → 7\n",
      "  25 TO 27 → 8\n",
      "  28 TO 30 → 9\n",
      "  31 TO 33 → 10\n",
      "  34 TO 36 → 11\n",
      "  37 TO 39 → 12\n",
      "  40 TO 42 → 13\n",
      "  43 TO 45 → 14\n",
      "  46 TO 48 → 15\n",
      "  49 TO 51 → 16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df_tree = df.copy()\n",
    "encoders = {}\n",
    "\n",
    "for col in ['town', 'flat_type', 'flat_model', 'storey_range']:\n",
    "    le = LabelEncoder()\n",
    "    df_tree[col] = le.fit_transform(df_tree[col])\n",
    "    encoders[col] = le\n",
    "\n",
    "    # Print mapping\n",
    "    print(f\"Encoding for '{col}':\")\n",
    "    for category, code in zip(le.classes_, le.transform(le.classes_)):\n",
    "        print(f\"  {category} → {code}\")\n",
    "    print()\n",
    "\n",
    "# Drop unused columns\n",
    "df_tree = df_tree.drop(columns=['month', 'coordinates', 'block', 'street_name', 'full_address'], errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daecede",
   "metadata": {},
   "source": [
    "<h3>Saving encoder for future use</h3>\n",
    "\n",
    "So that we can use it for mapping to display original values as selections in the frontend for UI/UX purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9b3cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_encoders.pkl']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save all encoders as a single .pkl file\n",
    "joblib.dump(encoders, './model/label_encoders.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f0d9ee",
   "metadata": {},
   "source": [
    "<h1>Conclusion</h1>\n",
    "\n",
    "We now have 3 sets of data:\n",
    "1. Full Dataset (final_dataset.csv): contains data from 2015-2025, preprocessed and added features.\n",
    "2. Dataset for Linear Modelling (dataset_for_linear.csv): dataset to use for linear models, one-hot and ordinally encoded.\n",
    "3. Dataset for Tree-based Modelling (dataset_for_tree.csv): dataset to use for tree models, encoded with sklearn's LabelEncoder.\n",
    "\n",
    "Next, we will be building and running models on the datasets in Models.ipynb."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SG Resale HDB Price Prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
